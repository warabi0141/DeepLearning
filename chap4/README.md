# 第4章まとめ

- ニューラルネットの学習は損失関数を指標として、それを最小化する様にパラメータを更新することで実現する
- 数値微分を使って重みパラメータの勾配を求めることができて、これによってパラメータの更新が可能（勾配降下法）
- 数値微分を使った勾配降下法は計算量が多くなってしまう。その課題を解決するために誤差逆伝播法を用いる
- ニューラルネットワークには「学習」と「推論」があるが、学習は次の手順で進む

1. (ミニバッチ)訓練データの中からランダムに一部のデータを選び出す
2. (勾配の算出)各重みパラメータに関する損失関数の勾配を求める
3. (パラメータの更新)重みパラメータを勾配方向に微小量だけ更新する
4. (繰り返す)以上3ステップを繰り返す

## 知ったこと

- 数値微分による勾配降下法だと、一回のループで1分かかった。重みの行列は784×50と50×10なので、一回のループでその成分の数だけ勾配を求めないといけないから大量の計算が必要になっている。
- なぜ認識精度ではなく損失関数を使うのか？その理由は、認識精度を使うと勾配が0になるところが多くなってしまうから。それによって学習が進まなくなってしまう。損失関数に使っている二乗和誤差や交差エントロピー誤差はいずれも凸関数で勾配降下法が適切に動作する（と思われる）。
