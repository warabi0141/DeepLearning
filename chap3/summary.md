
# 第3章のまとめ

- ニューラルネットワークは活性化関数としてシグモイド関数やReLU関数を利用する
- NumPyの多次元配列を使うことで、行列の積を用いてニューラルネットを表現できる
- 機械学習の問題は回帰と分類に分けられて、ニューラルネットは出力層の形を調整することでどちらにも対応できる
- 分類問題ではソフトマックス関数を使い、出力層のユニットの数をクラス数にする。
- 入力データのかたまりをバッチという。入力を単なるベクトルではなく、それらをまとめた行列にすることで、まとまったデータを一気に計算できる。行列計算は（病的な）最適化がなされているからある程度高速で、その速さを活かして一気に計算するほうが速い。

## 知らなかったこと

- 推論の段階でデータがニューラルネットを通って結果が出力される過程を**順方向伝播**(forward propagetion)という。back propagetionの逆。
- バッチ処理で高速化ができる
- バッチ処理をするときに

``` python
for i in range(0, len(x), batch_size):
    ...
```

と書くことで、batch_sizeの数だけとばしてループできる

- ソフトマックス関数を実装するときに、普通に実装するとexpの値が異常に大きくなって、(大きい数字)/(大きい数字)の計算をしないといけなくなって適切な計算がされない可能性がある。だからexpの引数を(a - (最大値))にするといい。
